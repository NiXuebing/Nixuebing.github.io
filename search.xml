<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[软件工程]]></title>
    <url>%2F2019%2F03%2F15%2F%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[瀑布模型 常见案例： 大瀑布拆小瀑布增量模型按模块分批次交付 迭代模型每次迭代都有一个可用的版本 用盖房子来理解，增量模型则是先盖厨房，再是卧室，这样一个个模块来完成。而迭代模型则是先盖一个简单的茅草房，有简易的土灶和土床，然后再升级成小木屋，有更好的灶和更好的卧室，这样一步步迭代成最终的房子。 敏捷开发 一切工作任务围绕 Ticket 开展 基于 Git 和 CI 的开发流程 自动测试与持续集成 每日站立会议 成员轮流发言（每个人轮流介绍一下，昨天干了什么事情，今天计划做什么事情，工作上有没有障碍无法推进。） 检查最新的Ticket 停车场问题 每周一个Sprint 每周一部署生产环境 每周二开迭代回顾会议，总结上一个Sprint 每周四迭代规划会，计划下周工作，评估每条Ticket的工作量（扑克估算） 每周五分支分割 该不该选择敏捷开发？ 团队要小，人数超过一定规模就要分拆； 团队成员之间要紧密协作，客户也要自始至终深度配合； 领导们得支持。敏捷需要扁平化的组织结构，更少的控制，更多的发挥项目组成员的主动性； 写代码时要有一定比例的自动化测试代码，要花时间搭建好源码管理和持续集成环境。 项目管理金三角 瀑布模型有严格的阶段划分，有需求分析、系统设计、开发和测试等阶段，通常在开发过程中不接受需求变更，也就是说，我们可以认为瀑布模型的范围是固定的，其他两条边时间和成本是变量。 敏捷开发中，是采用固定时间周期的开发模式，，例如每两周一个 Sprint，团队人数也比较少。所以，在敏捷开发中，时间和成本两条边是固定，就只有范围这条边是变量。]]></content>
      <categories>
        <category>软件工程</category>
      </categories>
      <tags>
        <tag>软件工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[指数基金阈值调整]]></title>
    <url>%2F2019%2F02%2F25%2F%E6%8C%87%E6%95%B0%E5%9F%BA%E9%87%91%E9%98%88%E5%80%BC%E8%B0%83%E6%95%B4%2F</url>
    <content type="text"><![CDATA[指数基金 按照规模划分： 中证100：最大的100只 （大盘） 中证200：最大的101-300只 （大盘） 沪深300：中证100 + 中证200 （大盘） 中证500：最大的301-800只 （中盘） 中证1000：最大的801-1800 （中小盘） 中证200对应指数：基本面60、基本面120 中证500对应指数：中证500、500增强、500低波动 大部分成分股也在此区间的指数：红利机会、养老产业 强周期性行业：证券、地产、环保、军工。 定投公式：金额 = 初始金额 * (进入低谷的市盈率/当前市盈率)^2]]></content>
      <categories>
        <category>理财</category>
      </categories>
      <tags>
        <tag>指数基金</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群搭建（一）]]></title>
    <url>%2F2019%2F01%2F22%2F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Zookeeper Rabbitmq安装ERLANG123456789101112vim /etc/yum.repos.d/rabbitmq-erlang.repo# In /etc/yum.repos.d/rabbitmq-erlang.repo[rabbitmq-erlang]name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq/rpm/erlang/20/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascrepo_gpgcheck=0enabled=1yum install erlang 安装rabbitmq注意Rabbitmq和Erlang的版本 12345wget https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.6/rabbitmq-server-3.7.6-1.el7.noarch.rpmrpm --import https://www.rabbitmq.com/rabbitmq-release-signing-key.ascyum install rabbitmq-server-3.7.6-1.el7.noarch.rpm 设置rabbitmq（单机）12345678910111213141516171819# 设置rabbitmq开机自启动chkconfig rabbitmq-server on# 新建rabbitmq.config，设置默认账号密码vi /etc/rabbitmq/rabbitmq.config[&#123;rabbit,[&#123;hipe_compile,false&#125;, &#123;tcp_listeners, [5670]&#125;, &#123;loopback_users,[]&#125;, &#123;default_user,&lt;&lt;&quot;root&quot;&gt;&gt;&#125;, &#123;default_pass,&lt;&lt;&quot;Flnet2018&quot;&gt;&gt;&#125;]&#125;,&#123;rabbitmq_management, [&#123;listener, [&#123;port, 15670&#125;]&#125;]&#125;].# 启动rabbitmqsystemctl start rabbitmq-serversystemctl status rabbitmq-server# 启动后台页面rabbitmq-plugins enable rabbitmq_management ElasticSearch安装ES123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.0.tar.gztar zxvf elasticsearch-5.5.0.tar.gz 配置ES12345vi config/elasticsearch.yml# 禁止消息自动创建索引action.auto_create_index: false# 禁止通配符删除索引action.destructive_requires_name: true 启动ES123456# ES 无法使用root，创建新用户useradd eschown -R es: elasticsearch-5.5.0/./elasticsearch -d 处理报错123456789101112max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]vim /etc/sysctl.confvm.max_map_count=655360sysctl -pmax file descriptors [65535] for elasticsearch process is too low, increase to at least [65536]vim /etc/security/limits.conf* soft nofile 65536* hard nofile 65536 Kibana安装Kibana1wget https://artifacts.elastic.co/downloads/kibana/kibana-5.5.0-linux-x86_64.tar.gz Netdata12345yum -y install zlib-devel libuuid-devel libmnl-devel gcc make git autoconf autogen automake pkgconfig curl jq nodejsgit clone https://github.com/firehol/netdata.git --depth=1./netdata-installer.sh --install /usr/local/server]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK-E学习笔记（三）]]></title>
    <url>%2F2019%2F01%2F10%2FELK-E%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[索引禁止自动创建索引，你 可以通过在 config/elasticsearch.yml 的每个节点下添加下面的配置： 1action.auto_create_index: false 使删除只限于特定名称指向的数据, 而不允许通过指定 _all 或通配符来删除指定索引库。 1action.destructive_requires_name: true 索引设置number_of_shards 每个索引的主分片数，默认值是 5 。这个配置在索引创建后不能修改。 number_of_replicas 每个主分片的副本数，默认值是 1 。对于活动的索引库，这个配置可以随时修改。 自定义分析器 https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-analyzers.html 根对象元数据：_source字段这个字段的存储几乎总是我们想要的，因为它意味着下面的这些： 搜索结果包括了整个可用的文档——不需要额外的从另一个的数据仓库来取文档。 如果没有 _source 字段，部分 update 请求不会生效。 当你的映射改变时，你需要重新索引你的数据，有了_source字段你可以直接从Elasticsearch这样做，而不必从另一个（通常是速度更慢的）数据仓库取回你的所有文档。 当你不需要看到整个文档时，单个字段可以从 _source 字段提取和通过 get 或者 search 请求返回。 调试查询语句更加简单，因为你可以直接看到每个文档包括什么，而不是从一列id猜测它们的内容。 元数据：_all字段一个把其它字段值当作一个大字符串来索引的特殊字段。 通过 include_in_all 设置来逐个控制字段是否要包含在 _all 字段中，默认值是 true。在一个对象(或根对象)上设置 include_in_all 可以修改这个对象中的所有字段的默认行为。 你可能想要保留 _all 字段作为一个只包含某些特定字段的全文字段，例如只包含 title，overview，summary 和 tags。 相对于完全禁用 _all 字段，你可以为所有字段默认禁用 include_in_all 选项，仅在你选择的字段上启用。 元数据：文档标识文档标识与四个元数据字段 相关： _id 文档的 ID 字符串 _type 文档的类型名 _index 文档所在的索引 _uid _type 和 _id 连接在一起构造成 type#id 默认情况下， _uid 字段是被存储（可取回）和索引（可搜索）的。 _type 字段被索引但是没有存储，_id 和 _index 字段则既没有被索引也没有被存储，这意味着它们并不是真实存在的。 尽管如此，你仍然可以像真实字段一样查询 _id 字段。Elasticsearch 使用 _uid 字段来派生出 _id 。 重新索引123456789POST _reindex&#123; "source": &#123; "index": "twitter" &#125;, "dest": &#123; "index": "new_twitter" &#125;&#125; 索引别名1234567POST /_aliases&#123; "actions": [ &#123; "remove": &#123; "index": "my_index_v1", "alias": "my_index" &#125;&#125;, &#123; "add": &#123; "index": "my_index_v2", "alias": "my_index" &#125;&#125; ]&#125; 分片新的文档被添加到内存缓冲区并且被追加到了事务日志 刷新（refresh）完成后, 缓存被清空但是事务日志不会 这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作。 这个段被打开，使其可被搜索。 内存缓冲区被清空。 事务日志不断积累文档 在刷新（flush）之后，段被全量提交，并且事务日志被清空 每隔一段时间–例如 translog 变得越来越大–索引被刷新（flush）；一个新的 translog 被创建，并且一个全量提交被执行 所有在内存缓冲区的文档都被写入一个新的段。 缓冲区被清空。 一个提交点被写入硬盘。 文件系统缓存通过 fsync 被刷新（flush）。 老的 translog 被删除。 translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。当 Elasticsearch 启动的时候， 它会从磁盘中使用最后一个提交点去恢复已知的段，并且会重放 translog 中所有在最后一次提交后发生的变更操作。 translog 也被用来提供实时 CRUD 。当你试着通过ID查询、更新、删除一个文档，它会在尝试从相应的段中检索之前， 首先检查 translog 任何最近的变更。这意味着它总是能够实时地获取到文档的最新版本。]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK-E学习笔记（二）]]></title>
    <url>%2F2019%2F01%2F10%2FELK-E%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[搜索空搜索1GET /_search 它简单地返回集群中所有索引下的所有文档。 hits 结果集：它 包含 total 字段来表示匹配到的文档总数，并且一个 hits 数组包含所查询结果的前十个文档。 took 执行整个搜索请求耗费了多少毫秒 shards 在查询中参与分片的总数，以及这些分片成功了多少个失败了多少个。正常情况下我们不希望分片失败，但是分片失败是可能发生的。 timeout 查询是否超时。默认情况下，搜索请求不会超时。 如果低响应时间比完成结果更重要，你可以指定 timeout 为 10 或者 10ms（10毫秒），或者 1s（1秒）： 1GET /_search?timeout=10ms 应当注意的是 timeout 不是停止执行查询，它仅仅是告知正在协调的节点返回到目前为止收集的结果并且关闭连接。在后台，其他的分片可能仍在执行查询即使是结果已经被发送了。 使用超时是因为 SLA(服务等级协议)对你是很重要的，而不是因为想去中止长时间运行的查询。 多索引，多类型1/_search 在所有的索引中搜索所有的类型 1/gb/_search 在 gb 索引中搜索所有的类型 1/gb,us/_search 在 gb 和 us 索引中搜索所有的文档 1/g*,u*/_search 在任何以 g 或者 u 开头的索引中搜索所有的类型 1/gb/user/_search 在 gb 索引中搜索 user 类型 1/gb,us/user,tweet/_search 在 gb 和 us 索引中搜索 user 和 tweet 类型 1/_all/user,tweet/_search 在所有的索引中搜索 user 和 tweet 类型 分页1GET /_search?size=5&amp;from=10 理解为什么深度分页是有问题的，我们可以假设在一个有 5 个主分片的索引中搜索。 当我们请求结果的第一页（结果从 1 到 10 ），每一个分片产生前 10 的结果，并且返回给 协调节点 ，协调节点对 50 个结果排序得到全部结果的前 10 个。 现在假设我们请求第 1000 页–结果从 10001 到 10010 。所有都以相同的方式工作除了每个分片不得不产生前10010个结果以外。 然后协调节点对全部 50050 个结果排序最后丢弃掉这些结果中的 50040 个结果。 可以看到，在分布式系统中，对结果排序的成本随分页的深度成指数上升。这就是 web 搜索引擎对任何查询都不要返回超过 1000 个结果的原因。 倒排索引一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 123456789101112131415161718Term Doc_1 Doc_2-----------------------Quick | | XThe | X |brown | X | Xdog | X |dogs | | Xfox | X |foxes | | Xin | | Xjumped | X |lazy | X | Xleap | | Xover | X | Xquick | X |summer | | Xthe | X |---------------------- 分析器字符过滤器 ： 一个字符过滤器可以用来去掉HTML，或者将 &amp; 转化成 and。 分词器 ： 一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。 Token过滤器 ： 这个过程可能会改变词条（例如，小写化 Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像 jump 和 leap 这种同义词）。 映射Elasticsearch 支持 如下简单域类型： 字符串: string 整数 : byte, short, integer, long 浮点数: float, double 布尔型: boolean 日期: date JSON type 域 type 布尔型: true 或者 false boolean 整数: 123 long 浮点数: 123.45 double 字符串，有效日期: 2014-09-15 date 字符串: foo bar string 自定义映射允许你执行下面的操作： 全文字符串域和精确值字符串域的区别 使用特定语言分析器 优化域以适应部分匹配 指定自定义数据格式 还有更多 域最重要的属性是 type 。对于不是 string 的域，你一般只需要设置 type string 域映射的两个最重要 属性是 index 和 analyzer 。（ES5后把string域拆分为keyword和text） index 属性控制怎样索引字符串。它可以是下面三个值： analyzed 首先分析字符串，然后索引它。换句话说，以全文索引这个域。 not_analyzed 索引这个域，所以它能够被搜索，但索引的是精确值。不会对它进行分析。 no 不索引这个域。这个域不会被搜索到。 对于 analyzed 字符串域，用 analyzer 属性指定在搜索和索引时使用的分析器。默认， Elasticsearch 使用 standard 分析器， 但你可以指定一个内置的分析器替代它，例如 whitespace 、 simple 和 english 尽管你可以 增加_ 一个存在的映射，你不能 _修改 存在的域映射。如果一个域的映射已经存在，那么该域的数据可能已经被索引。如果你意图修改这个域的映射，索引的数据可能会出错，不能被正常的搜索。 对于数组，没有特殊的映射需求。任何域都可以包含0、1或者多个值，就像全文域分析得到多个词条。 但是，数组是以多值域 索引的—可以搜索，但是无序的。 在搜索的时候，你不能指定 “第一个” 或者 “最后一个”。 对象：Lucene 不理解内部对象。文档是由一组键值对列表组成的。 为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样： 123456789&#123; "tweet": [elasticsearch, flexible, very], "user.id": [@johnsmith], "user.gender": [male], "user.age": [26], "user.name.full": [john, smith], "user.name.first": [john], "user.name.last": [smith]&#125; 内部对象的数组： 1234567&#123; "followers": [ &#123; "age": 35, "name": "Mary White"&#125;, &#123; "age": 26, "name": "Alex Jones"&#125;, &#123; "age": 19, "name": "Lisa Smith"&#125; ]&#125; 会被扁平化处理： 1234&#123; "followers.age": [19, 26, 35], "followers.name": [alex, jones, lisa, smith, mary, white]&#125; 过滤&amp;查询当使用于 过滤情况 时，查询被设置成一个“不评分”或者“过滤”查询。 当使用于 查询情况 时，查询就变成了一个“评分”的查询。 在一般情况下，一个filter 会比一个评分的query性能更优异，并且每次都表现的很稳定。 从 Elasticsearch 2.0 开始，过滤（filters）已经从技术上被排除了，同时所有的查询（queries）拥有变成不评分查询的能力。 match &amp; term 在一个全文字段上使用 match 查询，在执行查询前，它将用正确的分析器去分析查询字符串;在一个精确值的字段上使用它， 例如数字、日期、布尔或者一个 not_analyzed 字符串字段，那么它将会精确匹配给定的值。 term 查询被用于精确值 匹配，这些精确值可能是数字、时间、布尔或者那些 not_analyzed 的字符串。 相似度Elasticsearch 的相似度算法 被定义为检索词频率/反向文档频率， TF/IDF ，包括以下内容： 检索词频率 检索词在该字段出现的频率？出现频率越高，相关性也越高。 字段中出现过 5 次要比只出现过 1 次的相关性高。 反向文档频率 每个检索词在索引中出现的频率？频率越高，相关性越低。检索词出现在多数文档中会比出现在少数文档中的权重更低。 字段长度准则 字段的长度是多少？长度越长，相关性越低。 检索词出现在一个短的 title 要比同样的词出现在一个长的 content 字段权重更大。 分布式检索游标查询 游标查询会取某个时间点的快照数据。 查询初始化之后索引上的任何变化会被它忽略。 它通过保存旧的数据文件来实现这个特性，结果就像保留初始化时的索引 视图 一样。 注意游标查询每次返回一个新字段 _scroll_id。每次我们做下一次游标查询， 我们必须把前一次查询返回的字段 _scroll_id 传递进去。 当没有更多的结果返回的时候，我们就处理完所有匹配的文档了。]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK-E学习笔记（一）]]></title>
    <url>%2F2019%2F01%2F09%2FELK-E%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[基础入门一个 Elasticsearch 请求和任何 HTTP 请求一样由若干相同的部件组成： 1curl -X&lt;VERB&gt; &apos;&lt;PROTOCOL&gt;://&lt;HOST&gt;:&lt;PORT&gt;/&lt;PATH&gt;?&lt;QUERY_STRING&gt;&apos; -d &apos;&lt;BODY&gt;&apos; 被 &lt; &gt; 标记的部件： 参数 说明 VERB 适当的 HTTP 方法 或 谓词 : GET、 POST、 PUT、 HEAD 或者 DELETE。 PROTOCOL http 或者 https（如果你在 Elasticsearch 前面有一个 https 代理） HOST Elasticsearch 集群中任意节点的主机名，或者用 localhost 代表本地机器上的节点。 PORT 运行 Elasticsearch HTTP 服务的端口号，默认是 9200 。 PATH API 的终端路径（例如 _count 将返回集群中文档数量）。Path 可能包含多个组件，例如：_cluster/stats 和 _nodes/stats/jvm 。 QUERY_STRING 任意可选的查询字符串参数 (例如 ?pretty 将格式化地输出 JSON 返回值，使其更容易阅读) BODY 一个 JSON 格式的请求体 (如果请求需要的话) 底层原理一个运行中的 Elasticsearch 实例称为一个 节点，而集群是由一个或者多个拥有相同 cluster.name 配置的节点组成， 它们共同承担数据和负载的压力。当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。 索引保存相关数据的地方。 索引实际上是指向一个或者多个物理 分片 的 逻辑命名空间 。 分片一个底层的 工作单元 ，它仅保存了 全部数据中的一部分。一个分片是一个 Lucene 的实例，以及它本身就是一个完整的搜索引擎。 一个分片可以是 主 分片或者 副本 分片。 索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。 一个主分片最大能够存储 Integer.MAX_VALUE - 128 （2^31 -129） 个文档，但是实际最大值还需要参考你的使用场景：包括你使用的硬件， 文档的大小和复杂程度，索引和查询文档的方式以及你期望的响应时长。 主分片的数目在索引创建时 就已经确定了下来。实际上，这个数目定义了这个索引能够 存储 的最大数据量。（实际大小取决于你的数据、硬件和使用场景。） 但是，读操作——搜索和返回数据——可以同时被主分片 或 副本分片所处理，所以当你拥有越多的副本分片时，也将拥有越高的吞吐量。 文档一个文档的 _index 、 _type 和 _id 唯一标识一个文档。 文档 API： get 、 index 、 delete 、 bulk 、 update 以及 mget 路由一个文档到一个分片中shard = hash(routing) % number_of_primary_shards routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。 新建、索引和删除文档POST /index/type 创建新文档。 PUT /index/type/_id 创建一个新文档或者替换一个现有的文档。 DELETE /index/type_id 删除一个文档。 客户端向 Node 1 发送新建、索引或者删除请求。 节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node 3，因为分片 0 的主分片目前被分配在 Node 3 上。 Node 3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node 1 和 Node 2 的副本分片上。一旦所有的副本分片都报告成功, Node 3 将向协调节点报告成功，协调节点向客户端报告成功。 在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。 有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快 consistency consistency，即一致性。在默认设置下，即使仅仅是在试图执行一个_写_操作之前，主分片都会要求必须要有 规定数量(quorum)（或者换种说法，也即必须要有大多数）的分片副本处于活跃可用状态，才会去执行_写操作(其中分片副本可以是主分片或者副本分片)。这是为了避免在发生网络分区故障（network partition）的时候进行写操作，进而导致数据不一致。规定数量_即： int( (primary + number_of_replicas) / 2 ) + 1 consistency 参数的值可以设为 one （只要主分片状态 ok 就允许执行_写操作）,all（必须要主分片和所有副本分片的状态没问题才允许执行写操作）, 或 quorum 。默认值为 quorum , 即大多数的分片副本状态没问题就允许执行写_操作。 注意，规定数量 的计算公式中 number_of_replicas 指的是在索引设置中的设定副本分片数，而不是指当前处理活动状态的副本分片数。如果你的索引设置中指定了当前索引拥有三个副本分片，那规定数量的计算结果即： int( (primary + 3 replicas) / 2 ) + 1 = 3 如果此时你只启动两个节点，那么处于活跃状态的分片副本数量就达不到规定数量，也因此您将无法索引和删除任何文档。 timeout 如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 timeout 参数 使它更早终止： 100 100毫秒，30s 是30秒。 取回一个文档取回一个文档： GET _indx/_type/_id 取回文档的一部分：GET _index/_type/_id/_source=_field 客户端向 Node 1 发送获取请求。 节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node 2 。 Node 2 将文档返回给 Node 1 ，然后将文档返回给客户端。 在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。 局部更新文档POST /index/type/_id 部分更新一个文档。 客户端向 Node 1 发送更新请求。 它将请求转发到主分片所在的 Node 3 。 Node 3 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃。 如果 Node 3 成功地更新文档，它将新版本的文档并行转发到 Node 1 和 Node 2 上的副本分片，重新建立索引。 一旦所有副本分片都返回成功， Node 3 向协调节点也返回成功，协调节点向客户端返回成功。 当主分片把更改转发到副本分片时， 它不会转发更新请求。 相反，它转发完整文档的新版本。 在 Elasticsearch 中文档是 不可改变 的，不能修改它们。 相反，如果想要更新现有的文档，需要 重建索引或者进行替换。 调用POST /index/type/_id做部分更新的好处在于这个过程发生在分片内部，这样就避免了多次请求的网络开销。通过减少检索和重建索引步骤之间的时间，我们也减少了其他进程的变更带来冲突的可能性。 在使用spring-boot-starter-data-elasticsearch时，由于API都被封装了，不确定实际调用的是index API还是update API，有待具体确认。 mget 取回多个文档 客户端向 Node 1 发送 mget 请求。 Node 1 为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复， Node 1 构建响应并将其返回给客户端。 https://www.elastic.co/guide/cn/elasticsearch/guide/current/_Retrieving_Multiple_Documents.html bulk 批量操作 客户端向 Node 1 发送 bulk 请求。 Node 1 为每个节点创建一个批量请求，并将这些请求并行转发到每个包含主分片的节点主机。 主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。 整个批量请求都需要由接收到请求的节点加载到内存中，因此该请求越大，其他请求所能获得的内存就越少。 批量请求的大小有一个最佳值，大于这个值，性能将不再提升，甚至会下降。 但是最佳值不是一个固定的值。它完全取决于硬件、文档的大小和复杂度、索引和搜索的负载的整体情况。 幸运的是，很容易找到这个 最佳点 ：通过批量索引典型文档，并不断增加批量大小进行尝试。 当性能开始下降，那么你的批量大小就太大了。一个好的办法是开始时将 1,000 到 5,000 个文档作为一个批次, 如果你的文档非常大，那么就减少批量的文档个数。 密切关注你的批量请求的物理大小往往非常有用，一千个 1KB 的文档是完全不同于一千个 1MB 文档所占的物理大小。 一个好的批量大小在开始处理后所占用的物理大小约为 5-15 MB。 https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html 并发冲突 乐观并发控制 https://www.elastic.co/guide/cn/elasticsearch/guide/current/optimistic-concurrency-control.html]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[压力测试（一）]]></title>
    <url>%2F2019%2F01%2F02%2F%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、服务器性能指标平均负载、CPU、内存指标：平均负载不高于CPU数量70%，CPU和内存使用量不超过80% 监控方法： top，free -h 查看实时的CPU、内存等多项指标 watch -d uptime 查看平均负载 mpstat -P ALL 5 监控所有 CPU，后面数字 5 表示间隔5秒输出一组数据 网络IO使用ifconfig查看服务器正在使用的网卡 监控方法： nload eth0 二、压测指标平均响应时间、中位数响应时间可能由于部分请求响应过慢会导致平均响应时间相对偏大，相对而言中位数稍微靠谱一点。 TP90、TP95、TP99用百分比分布统计。 吞吐量系统的性能如果只看吞吐量，不看响应时间是没有意义的。我的系统可以顶10万请求，但是响应时间已经到了5秒钟，这样的系统已经不可用了，这样的吞吐量也是没有意义的。 所以，吞吐量的值必需有响应时间来卡。比如：TP99小于100ms的时候，系统可以承载的最大并发数是1000qps。这意味着，我们要不断的在不同的并发数上测试，以找到软件的最稳定时的最大吞吐量。 成功率保证系统稳定性 压测流程一、定义一个系统的响应时间latency，建议是TP99，以及成功率。 二、在这个响应时间的限制下，找到最高的吞吐量。 三、在最大吞吐量下做浸泡测试，观察CPU、内存、网络IO等。如果指标平稳，这个值就是系统性能。 四、找系统极限值，在保证成功率100%，不考虑响应时长的情况下，系统能坚持10分钟的吞吐量。 五、用第二步得到的吞吐量执行5分钟，然后在第四步得到的极限值执行1分钟，再回到第二步的吞吐量执行5钟，再到第四步的权限值执行1分钟，如此往复个一段时间，比如2天。收集系统数据：CPU、内存、硬盘/网络IO等，观察他们的曲线，以及相应的响应时间，确保系统是稳定的。 压测方法一、 确定测试接口和接口逻辑，构造请求参数和jmeter脚本。本次以空接口为例。 Jmeter脚本： 二、指定响应时间指标。 TP99 &lt; 50ms，成功率 100% 三、上传jmeter脚本 ./jmeter -n -r -t /root/test02.jmx -l /root/02.jtl -j /root/02.log 四、本地jmeter加载结果分析]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>压力测试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记（三）]]></title>
    <url>%2F2018%2F11%2F26%2FMysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[全局锁加全局读锁的命令：Flush tables with read lock (FTWRT)。 整个库处于只读状态，阻塞数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 适用于不支持事务的引擎做全库逻辑备份的场景 InnoDB做全库备份的方法： 官方自动的逻辑备份工具mysqldump使用参数-single-transaction，在可重复读隔离级别下开启一个事务，确保那个一致性视图。 表级锁表锁加表锁的命令：lock tables ... read/write。 表锁是最常用的处理并发的方式，但对于InnoDB这种支持行锁的引擎，一般不使用lock tables来控制并发。 元数据锁（MDL）MDL(metadata lock)不需要显示使用，在访问一个表的时候会被自动加上。 当对一个表做增删改查操作的时候，加MDL读锁 当要对表做结构变更操作的时候，加MDL写锁 读锁之间不互斥，可以用多个线程同时对一张表增删改查 读写锁之间，写锁之间是互斥的，用来保证变更表结构操作的安全性 长事务导致改表数据库崩溃问题 A存在长事务，session C的MDL写锁互斥会block住，后续的读锁也会因为session C的写锁阻塞住。 因此必须解决长事务，事务不提交，就会一直占着MDL锁。 但对于热点表，会不停有新的请求，无法kill掉事务，可以在alter table语句里设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。 12ALTER TABLE tbl_name NOWAIT add column ...ALTER TABLE tbl_name WAIT N add column ...]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库事务</tag>
        <tag>数据库锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记（二）]]></title>
    <url>%2F2018%2F11%2F25%2FMysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[索引相关概念回表12345678mysql&gt; create table T ( ID int primary key,k int NOT NULL DEFAULT 0,s varchar(16) NOT NULL DEFAULT &apos;&apos;,index k(k))engine=InnoDB;insert into T values(100, 1, &apos;aa&apos;),(200, 2, &apos;bb&apos;),(300, 3, &apos;cc&apos;),(500, 5, &apos;ee&apos;),(600, 6, &apos;ff&apos;),(700, 7, &apos;gg&apos;); 执行语句select * from T where k between 3 and 5 在k索引树上找到k=3的记录，取得ID=300； 再到ID索引树查到ID=300对应的R3； 在k索引树取下一个值k=5，取得ID=500； 再回到ID索引树查到ID=500对应的R4； 在k索引树取下一个值k=6，不满足条件，循环结束。 这个查询过程读取了k索引树的3条记录(步骤1、3、5)，回表了两次(步骤2、4)。 覆盖索引如果执行的语句是select ID from T where k between 3 and 5，ID为主键已经在k索引树上了，因此可以直接提供查询结果，不需要回表。 由于覆盖索引可以减少树的搜索次数，可以显著提升查询性能。 最左前缀原则用(name, age)这个联合索引分析如果要查的是所有名字第一个字是“张”的人，执行的SQL语句的条件是where name like &#39;张%&#39;，也能够用上这个索引。 可以看到，不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符。 第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。 索引下推select * from tuser where name like ‘张%’ and age=10 and ismale=1;通过最左前缀原则，过滤出“张”开头的索引，再判断age=10`再过滤一遍再做回表操作。 MySQL 5.6后引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记（一）]]></title>
    <url>%2F2018%2F11%2F25%2FMysql%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[MySQL基础架构 连接器连接器负责跟客户端建立连接、获取权限、维持和管理连接。1mysql -h$ip -P$port -u$user -p 连接完成后，如果没有后续的操作，这个连接就处于空闲状态。客户端如果太长时间没动静，连接器就会自动断开连接，默认时长8小时，由wait_timeout参数控制。 建立连接的过程通常比较复杂，TCP握手，权限认证等，因此使用中尽量减少建立连接的动作，即尽量使用长连接而不是短连接。 注意：大量使用长连接，可能导致内存占用太大，被系统强行杀掉(OOM) 查询缓存MySQL会以键值对的形式直接缓存到内存中，key是查询语句，value是查询的结果。通常不建议使用查询缓存，因为缓存失效非常频繁，总要对一个表更新，这个表上的所有查询缓存都会清空，更新压力大的数据库，查询缓存的命中率非常低 设置参数query_cache_type = DEMAND，这样对于默认的SQL语句都不使用查询缓存。实际开发用mybatis的话会用到一级缓存和二级缓存，并不需要使用MySQL的查询缓存。 Spring又会使mybatis的一级缓存失效，二级缓存又存在弊端，所以使用时一定要结合实际场景。 注意：MySQL 8.0版本直接将查询缓存的整块功能删除了 分析器分析器会做”词法分析“和”语法分析“。如当执行语句select * from T where K = 1;报不存在这个列，就是分析器阶段的处理结果。 优化器优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联(join)的时候，决定各个表的连接顺序。 执行器开始执行的时候，会先判断一下对表是否有执行权限，如果没有，就会返回没有权限的错误。如果有，就打开表调用引擎接口进行执行。 MySQL日志模块redo log当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里，并更新到内存，等系统比较空闲的时候，InnoDB引擎会将这个操作记录更新到磁盘里。 这个操作使得InnoDB可以保证即使数据库发生异常重启，之前提交的记录也不会丢失，即crash-safe能力。 redo log 是 InnoDB引擎特有的，是物理日志，记录了”在某个数据页做了什么操作。“ redo log 是循环写的，通过write pos 记录当前记录位置，checkpoint记录擦除位置。当write pos 追上 checkpoint时，停下来擦除部分记录。 innodb_flush_log_at_trx_commit参数设置为1，表示每次事务的redo log都直接持久化到磁盘。 binlogbinlog 是MySQL server层实现的，所有引擎都可以使用。记录的是逻辑日志。分两张模式，statement格式的话是记SQL语句，row格式会记录行的内容，记两条，更新前和更新后都有。 sync_binlog参数设置为1，表示每次事务的binlog都持久化到磁盘。 由于binlog日志只能用于归档，InnoDB结合redo log 来实现 crash-safe 能力。 图中浅色框表示在InnoDB内部执行，深色框表示是在执行器中执行的。 1、 prepare阶段 2、 写binlog 3、commit当在2之前崩溃时重启恢复redo log发现没有commit，binlog没有，事务回滚。当在3之前崩溃时重启恢复redo log发现没有commit，binlog有，认可事务，自动提交。 数据库还原 首先找到最近的一次全量备份，恢复到临时库。 从备份的时间点开始，依次重放binlog记录到需要还原的时间点 MySQL事务事务隔离级别SQL 标准的事务隔离级别 读未提交(read uncommitted) 读提交(read committed) 可重复读(repeatable read) 串行化(serializabel) 12mysql&gt; create table T(c int) engine=InnoDB;insert into T(c) values(1); 读未提交一个事务还没提交时，它做的变更就能被别的事务看到。若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务B虽然还没有提交，但结果已经被A看到了。因此，V2、V3也都是2 读提交一个事务提交之后，它做的变更才会被其他事务看到。若隔离级别是”读提交“，则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以，V3的值也是2。 可重复读一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据一致。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。若隔离级别是”可重复读“，则V1、V2是1，V3是2。之所以V2还是1，遵循的就是事务在执行期间看到的数据前后必须是一致的。 串行化对于同一行记录，”写“会加”写锁“，”读“会加”读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。若隔离级别是“串行化”，则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看，V1，V2值是1，V3的值是2。 查看方式 1234567891011mysql&gt; show variables like &apos;transaction_isolation&apos;;+-----------------------+----------------+| Variable_name | Value |+-----------------------+----------------+| transaction_isolation | READ-COMMITTED |+-----------------------+----------------+ 长事务使用set autocommit=1，通过显示语句的方式启动事务。对于一个需要频繁使用事务的业务，可以执行commit work and chain，提交事务并自动启动下一个事务。 12# 查询持续时间超过60s的长事务select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(), trx_started)) &gt; 60; MySQL索引索引常见数据模型哈希表哈希表适用于只有等值无序查询的场景，比如Memcached及其他一些NoSQL引擎。 有序数组可以通过二分法快速查找，查询效率高，但更新成本太高，只适用于静态存储引擎。比如存2017年的数据这样的不会再修改的数据。 搜索树N叉树由于在读写上的性能优点，以及适配磁盘的访问模式，已经被广泛应用在数据库引擎中了。 索引类型索引类型分为主键索引（聚簇索引）和非主键索引。主键索引的叶子节点存放的是整行数据。非主键索引的叶子节点存放的是主键的值。 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 索引重建索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间。 重建主键索引不需要通过删除主键再创建主键的方式，因为不论删除主键还是创建主键，都会将整个表重建，所以连着执行这个两个语句，第一个删除主键的操作就白做了。可以通过这个语句替换：alter table T engine=InnoDB。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>数据库索引</tag>
        <tag>数据库事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux性能学习]]></title>
    <url>%2F2018%2F11%2F24%2Flinux%E6%80%A7%E8%83%BD%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[性能学习基础性能指标是什么高并发 – 吞吐 响应快 – 延时 Linux 性能工具图谱 linux性能思维导图 平均负载单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数。 可运行状态进程：正在使用CPU或者正在等待CPU的进程 – 常用ps命令可以看到的，处于R状态(Running 或 Runable) 的进程。 不可中断状态进程：正处于内核态关键流程中的进程，并且这些流程是不可打断的。最常见的是等待硬件设备的I/O响应，常用ps命令看到的，处于D状态的进程。 当平均负载高于CPU数量70%的时候，就应该分析排查负载高的问题了。 平均负载 ≠ CPU使用率 CPU密集型进程，使用大量CPU会导致平均负载升高，CPU使用率也高 I/O密集型进程，等待I/O也会导致平均负载升高，但CPU使用率不一定高 大量等待CPU的进程调度也会导致平均负载和CPU使用率升高 压力分析stress： linux系统压力测试工具。sysstat：包含常用的linux性能工具，用来监控和分析系统性能。包含 mpstat和pidstat两个命令。 mpstat 用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标 pidstat 用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标 12root$ uptime11:17:07 up 264 days, 14:21, 4 users, load average: 0.06, 0.12, 0.13 CPU 密集型进程123456789101112131415161718192021#第一个终端运行stress命令，模拟一个CPU使用率100%的场景root$ stress --cpu 1 --timeout 600stress: info: [20102] dispatching hogs: 1 cpu, 0 io, 0 vm, 0 hdd#第二个终端运行uptime查看平均负载# -d 参数表示高亮显示变化的区域root$ watch -d uptime11:24:38 up 264 days, 14:28, 4 users, load average: 1.08, 0.67, 0.3# 一分钟的平均负载已慢慢升到1.0.8#第三个终端运行mpstat查看CPU使用情况# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔5秒输出一组数据root$ mpstat -P ALL 5Linux 3.10.0-693.2.2.el7.x86_64 (TV-DEV-DB01) 2018年11月25日 _x86_64_ (4 CPU)11时20分51秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle11时20分56秒 all 25.66 0.00 0.35 0.55 0.00 0.05 0.00 0.00 0.00 73.3811时20分56秒 0 0.80 0.00 0.40 0.00 0.00 0.20 0.00 0.00 0.00 98.5911时20分56秒 1 0.81 0.00 0.40 2.02 0.00 0.00 0.00 0.00 0.00 96.7711时20分56秒 2 0.60 0.00 0.40 0.00 0.00 0.20 0.00 0.00 0.00 98.8011时20分56秒 3 99.80 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0.00 0.00 看到一个CPU的使用率为100%，但它的iowait只有0。12345678910111213141516# 间隔5秒输出一组数据root$ pidstat -u 5 1Linux 3.10.0-693.2.2.el7.x86_64 (TV-DEV-DB01) 2018年11月25日 _x86_64_ (4 CPU)11时32分23秒 UID PID %usr %system %guest %CPU CPU Command11时32分28秒 0 940 0.20 0.20 0.00 0.40 3 AliYunDun11时32分28秒 0 6164 0.40 0.00 0.00 0.40 2 java11时32分28秒 0 6483 0.00 0.20 0.00 0.20 1 aliyun-service11时32分28秒 0 14754 0.20 0.00 0.00 0.20 3 java11时32分28秒 0 20104 0.20 0.20 0.00 0.40 3 watch11时32分28秒 0 20826 100.00 0.00 0.00 100.00 1 stress11时32分28秒 0 25477 0.20 0.40 0.00 0.60 2 java11时32分28秒 0 25718 0.20 0.00 0.00 0.20 1 java11时32分28秒 0 28889 1.00 0.60 0.00 1.60 1 java11时32分28秒 0 30891 0.20 0.20 0.00 0.40 0 mongod11时32分28秒 0 32076 0.00 0.20 0.00 0.20 3 redis-server 可以看到，stress进程的CPU使用率为100%。 I/O密集型进程12345678910111213141516#第一个终端运行stress命令，模拟一个I/O压力的场景root$ stress -i 1 --timeout 600#第二个终端运行uptime查看平均负载root$ watch -d uptime#第三个终端运行mpstat查看CPU使用情况root$ mpstat -P ALL 5Linux 3.10.0-693.2.2.el7.x86_64 (TV-DEV-DB01) 2018年11月25日 _x86_64_ (4 CPU)11时58分45秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle11时58分50秒 all 0.86 0.00 16.56 5.50 0.00 0.00 0.00 0.00 0.00 77.0811时58分50秒 0 0.40 0.00 52.92 10.06 0.00 0.00 0.00 0.00 0.00 36.6211时58分50秒 1 0.40 0.00 1.01 1.82 0.00 0.00 0.00 0.00 0.00 96.7711时58分50秒 2 2.03 0.00 10.14 9.53 0.00 0.00 0.00 0.00 0.00 78.3011时58分50秒 3 0.80 0.00 1.61 0.80 0.00 0.20 0.00 0.00 0.00 96.58 使用pidstat查看，会发现还是stress进程导致的。使用top命令查看 大量进程的场景当系统中运行的进程超出CPU运行能力时，就会出现等待CPU的进程。12345root$ stress -c 16 --timeout 600# 系统只有4个CPU，少于16个进程root$ watch -d uptime12:06:15 up 264 days, 15:10, 4 users, load average: 14.90, 5.78, 2.35 使用top命令查看]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>平均负载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法学习之路]]></title>
    <url>%2F2018%2F11%2F24%2F%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%B7%AF%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 安装笔记]]></title>
    <url>%2F2018%2F11%2F24%2FHexo-%E5%AE%89%E8%A3%85%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[重拾Hexo过了大半年，线上买的阿里云服务器到期了，又不打算续了，考虑原来的几篇博文还在上面，就做了个迁移，并且重拾了下Hexo。 安装Hexo检查环境配置1234node -Vnpm -vgit --versionhexo -v 未安装hexo，通过npm安装1npm install -g hexo-cli 初始化Hexo12345$ mkdir hexo_blog$ cd hexo_blog$ hexo init$ npm install$ hexo g s 默认主题是landscape，不是那么高大上，所以切换next主题1234567891011$ cd themes$ git clone https://github.com/iissnan/hexo-theme-next themes/next$ cd ..$ vi _config.yml找到 theme: landscape修改为 theme: next$ hexo clean$ hexo g$ hexo s next主题的定制化这里不再赘述。 关联GitHub 和 Coding1234567$ vi _config.ymldeploy: type: git repository: github: https://github.com/your/your.github.io.git,master coding: https://git.coding.net/your/Hexo_Blog.git,master 如存在账号问题可以配置.git/config尝试解决。 源码上传GitHub hexo分支1234567$ git init开一个hexo新分支$ git checkout -b hexo$ git add .$ git commit -m 'hexo init'$ git remote add origin https://github.com/your/your.github.io.git$ git push origin hexo:hexo master 存放发布的博文，hexo分支存放源码文件 编辑实时预览通过hexo-browsersync和 hexo s实现编辑实时预览功能由于不同markdown工具的可视化预览功能效果各有差异，我选择了网页实时预览看效果。12$ npm install hexo-browsersync --save$ hexo s 打开谷歌浏览器 http://localhost:4000/ 即可实时预览。 编辑器选择 Typora，开启源码模式，专注模式，打字机模式，不用按Ctrl + S 写起来真的很爽。图片上传选择iPic，使用默认的微博图床，有条件可以付费接各种云，微信截图自动压缩上传生成markdown图片链接，复制即可插入。]]></content>
      <categories>
        <category>工具记录</category>
      </categories>
      <tags>
        <tag>Hexo Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 学习笔记（一）]]></title>
    <url>%2F2018%2F06%2F28%2FDocker-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Linux 容器不是模拟一个完整的操作系统，而是对进程进行隔离。 启动快 占用资源少 体积小 Docker 属于 Linux 容器的一种封装，提供简单易用的容器使用接口。 提供一次性的环境 提供弹性的云服务 组建微服务架构 123456789101112131415161718192021222324252627# Uninstall old versions$ yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine# Install Docker CE$ yum install -y yum-utils device-mapper-persistent-data lvm2$ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo$ yum-config-manager --enable docker-ce-edge$ yum-config-manager --enable docker-ce-test$ yum install docker-ce$ systemctl start docker$ docker version$ docker info Docker 把应用程序及其依赖，打包在 image 文件里面。 12345# 列出本机的所有 image 文件。$ docker image ls# 删除 image 文件$ docker image rm [imageName] image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。 1234567# 配置国内镜像仓库$ vi /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://registry.docker-cn.com&quot;]&#125;$ systemctl restart docker docker container run 命令会从 image 文件，生成一个正在运行的容器实例。 注意，docker container run 命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。 对于那些不会自动终止的容器，必须使用docker container kill [containerID] 命令手动终止。 image 文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。 12345# 列出本机正在运行的容器$ docker container ls# 列出本机所有容器，包括终止运行的容器$ docker container ls --all 终止运行的容器文件，依然会占据硬盘空间，可以使用docker container rm [containerID] 命令删除。 RUN 命令在 image 文件的构建阶段执行，执行结果都会打包进入 image 文件；CMD 命令则是在容器启动后执行。另外，一个 Dockerfile 可以包含多个RUN命令，但是只能有一个CMD命令 （1）docker container start 前面的docker container run命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用docker container start命令，它用来启动已经生成、已经停止运行的容器文件。 1$ docker container start [containerID] （2）docker container stop 前面的docker container kill命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而docker container stop命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号。 1$ docker container stop [containerID] 这两个信号的差别是，应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。 （3）docker container logs docker container logs命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令查看输出。 1$ docker container logs [containerID] （4）docker container exec docker container exec命令用于进入一个正在运行的 docker 容器。如果docker run命令运行容器的时候，没有使用-it参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。1$ docker container exec -it [containerID] /bin/bash （5）docker container cp docker container cp命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。 1$ docker container cp [containID]:[/path/to/file] .]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 学习笔记（二）]]></title>
    <url>%2F2018%2F05%2F03%2FKafka-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[kafka核心组件主要包括延长操作组件、控制器、协调器、网络通信、日志管理器、副本管理器、动态配置管理器及心跳检测等。 一、延长操作组件DelayedOperation一个基于事件启动有失效时间的TimerTask。]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK集群搭建（二）]]></title>
    <url>%2F2018%2F04%2F14%2FELK%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Elasticsearch 破解本来是想先整logstash订阅kafka集群清洗日志到elasticsearch，但是突然发现kibana显示x-pack一个月过期。百度一下果然有破解方法，这一下强迫症就犯了，不破解不舒服斯基。蛋疼的是网上资料基本都是5版本的，最高也就6.0版本，和我现在用的6.2.3的版本还是有点差距，果然一路趟坑，花了一天时间终于爬完了破解之旅。 反编译反编译肯定先找到源码，网上的资料基本都是 /usr/local/elasticsearch/plugins/x-pack/x-pack-6.0.0.jar， 一看，6.2.3版果然没有x-pack-6.2.3.jar，不过在x-pack-core下有个x-pack-core-6.2.3.jar，解压一看正好有org/elasticsearch/license/LicenseVerifier.class，这就可以玩下去了。。。 123456789101112131415cd /data/local# 新建测试目录mkdir test# 剪切到测试目录cp /usr/local/server/elasticsearch-6.2.3/plugins/x-pack/x-pack-core/x-pack-core-6.2.3.jar test/# 切换到测试目录cd test# 解压jar包jar -xvf x-pack-core-6.2.3.jar# 移除jar包rm x-pack-core-6.2.3.jar 找到org/elasticsearch/license/LicenseVerifier.class，使用Luyten反编译（用的Mac版，不知道Windows是不是一样） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package org.elasticsearch.license;import java.nio.*;import java.util.*;import java.security.*;import org.elasticsearch.common.xcontent.*;import org.apache.lucene.util.*;import org.elasticsearch.common.io.*;import java.io.*;public class LicenseVerifier&#123; public static boolean verifyLicense(final License license, final byte[] encryptedPublicKeyData) &#123; byte[] signedContent = null; byte[] signatureHash = null; try &#123; final byte[] signatureBytes = Base64.getDecoder().decode(license.signature()); final ByteBuffer byteBuffer = ByteBuffer.wrap(signatureBytes); final int version = byteBuffer.getInt(); final int magicLen = byteBuffer.getInt(); final byte[] magic = new byte[magicLen]; byteBuffer.get(magic); final int hashLen = byteBuffer.getInt(); signatureHash = new byte[hashLen]; byteBuffer.get(signatureHash); final int signedContentLen = byteBuffer.getInt(); signedContent = new byte[signedContentLen]; byteBuffer.get(signedContent); final XContentBuilder contentBuilder = XContentFactory.contentBuilder(XContentType.JSON); license.toXContent(contentBuilder, (ToXContent.Params)new ToXContent.MapParams((Map)Collections.singletonMap("license_spec_view", "true"))); final Signature rsa = Signature.getInstance("SHA512withRSA"); rsa.initVerify(CryptUtils.readEncryptedPublicKey(encryptedPublicKeyData)); final BytesRefIterator iterator = contentBuilder.bytes().iterator(); BytesRef ref; while ((ref = iterator.next()) != null) &#123; rsa.update(ref.bytes, ref.offset, ref.length); &#125; return rsa.verify(signedContent) &amp;&amp; Arrays.equals(Base64.getEncoder().encode(encryptedPublicKeyData), signatureHash); &#125; catch (IOException ex) &#123;&#125; catch (NoSuchAlgorithmException ex2) &#123;&#125; catch (SignatureException ex3) &#123;&#125; catch (InvalidKeyException e) &#123; throw new IllegalStateException(e); &#125; finally &#123; Arrays.fill(encryptedPublicKeyData, (byte)0); if (signedContent != null) &#123; Arrays.fill(signedContent, (byte)0); &#125; if (signatureHash != null) &#123; Arrays.fill(signatureHash, (byte)0); &#125; &#125; &#125; public static boolean verifyLicense(final License license) &#123; byte[] publicKeyBytes; try (final InputStream is = LicenseVerifier.class.getResourceAsStream("/public.key")) &#123; final ByteArrayOutputStream out = new ByteArrayOutputStream(); Streams.copy(is, (OutputStream)out); publicKeyBytes = out.toByteArray(); &#125; catch (IOException ex) &#123; throw new IllegalStateException(ex); &#125; return verifyLicense(license, publicKeyBytes); &#125;&#125; 拷贝内容到新建的文件LicenseVerifier.java，两个方法返回值改为true 1234567891011121314151617181920package org.elasticsearch.license;import java.nio.*;import java.util.*;import java.security.*;import org.elasticsearch.common.xcontent.*;import org.apache.lucene.util.*;import org.elasticsearch.common.io.*;import java.io.*;public class LicenseVerifier&#123; public static boolean verifyLicense(final License license, final byte[] encryptedPublicKeyData) &#123; return true; &#125; public static boolean verifyLicense(final License license) &#123; return true; &#125;&#125; 对LicenseVerifier.java进行重新编译，需要依赖三个包123javac -cp "/usr/local/server/elasticsearch-6.2.3/lib/elasticsearch-6.2.3.jar:/usr/local/server/elasticsearch-6.2.3/lib/lucene-core-7.2.1.jar:/usr/local/server/elasticsearch-6.2.3/plugins/x-pack/x-pack-core/x-pack-core-6.2.3.jar" LicenseVerifier.java 生成LicenseVerifier.class 替换原来的class文件，重新打包12jar -cvf x-pack-core-6.2.3.jar ./*mv x-pack-core-6.2.3.jar /usr/local/server/elasticsearch-6.2.3/plugins/x-pack/x-pack-core/ 至此破解完成，多台ES则把x-pack-core-6.2.3.jar都替换掉，先别急着启动，一启一个坑 注册在线注册，成功会收到邮件, 有下载license.json的链接。123456789101112&#123;&quot;license&quot;:&#123; &quot;uid&quot;:&quot;xxx&quot;, &quot;type&quot;:&quot;platinum&quot;, &quot;issue_date_in_millis&quot;:1515024000000, &quot;expiry_date_in_millis&quot;:1596646399999, &quot;max_nodes&quot;:100, &quot;issued_to&quot;:&quot;aaa&quot;, &quot;issuer&quot;:&quot;Web Form&quot;, &quot;signature&quot;:&quot;111&quot;, &quot;start_date_in_millis&quot;:1515024000000 &#125;&#125; type 修改为 platinum(白金版)expiry_date_in_millis 修改为 2524579200999 (2050年) 修改elasticsearch.ymlx-pack白金版必须启动SSL，就必须要配置CA证书，然后就是一堆坑，p12格式的证书一直跑不通，只能用key+crt123bin/x-pack/certgen# 根据提示 填写输出的证书名，节点名，ip，DNS等，多个节点则最后一步选Y，进行下一个节点证书的生成，出错了没关系，删了生成的zip包重新再来就好了 生成好证书后就解压放到合理位置就好了，elasticsearch.yml里指定路径，我的是放在config/certs下， 配置如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#cluster.name: dev-elasticsearch## ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#node.name: node-2## Add custom attributes to the node:#node.attr.rack: r2## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):#path.data: /data/share/server_data/mq_02/elasticsearch## Path to log files:#path.logs: /data/share/server_log/mq_02/elasticsearch## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:##bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#network.host: 172.18.1.153## Set a custom port for HTTP:#http.port: 9200## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when new node is started:# The default list of hosts is ["127.0.0.1", "[::1]"]#discovery.zen.ping.unicast.hosts: ["172.18.1.152", "172.18.1.153", "172.18.1.154"]## Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):##discovery.zen.minimum_master_nodes: 2## For more information, consult the zen discovery module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------xpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.ssl.key: certs/$&#123;node.name&#125;/$&#123;node.name&#125;.keyxpack.ssl.certificate: certs/$&#123;node.name&#125;/$&#123;node.name&#125;.crtxpack.ssl.certificate_authorities: certs/ca/ca.crt 破解首先清理path.data下的nodes数据，从头开始，启动第一个节点1234567891011121314bin/elasticsearch#应该是提示需要配置密码，另起窗口进行密码配置bin/x-pack/set-passwords interactive#配置完密码可以先查看当前权限curl -XGET -u elastic:XXX 'http://172.18.1.152:9200/_xpack/license'#发送licence.json修改权限，在license.json同级目录运行curl -XPUT -u elastic:XXX 'http://172.18.1.152:9200/_xpack/license?acknowledge=true' -H "Content-Type: application/json" -d @license.json#再次查看权限应该已经修改为platinumcurl -XGET -u elastic:XXX 'http://172.18.1.152:9200/_xpack/license' 集群启动配好证书和elasticsearch.yml后启动节点就行，会自动进行选举和复制，无需再次破解。]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELK集群搭建（一）]]></title>
    <url>%2F2018%2F04%2F13%2FELK%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[ELK简介ELK Stack 是软件集合 Elasticsearch、Logstash、Kibana 的简称，由这三个软件及其相关的组件可以打造大规模日志实时处理系统。 E - Elasticsearch Elasticsearch 是基于 JSON 的分布式搜索和分析引擎，专为实现水平扩展、高可用和管理便捷性而设计。 L - Logstash Logstash 是开源的服务器端数据处理管道，能够同时 从多个来源采集数据、转换数据，然后将数据发送到您最喜欢的 “存储库” 中。 K - Kibana Kibana 能够以图表的形式呈现数据，并且具有可扩展的用户界面，供您全方位配置和管理 Elastic Stack。 ELK使用场景 日志分析 – 快速、可扩展的日志记录分析 网站搜索 – 创建良好的搜索体验 安全分析 – 快速且规模化的交互式调查 APM – 深入了解应用程序的性能 应用搜索 – 搜索文档、地理数据等 ELK产品介绍 搭建准备 CentOS7-mq01: 172.18.1.152 CentOS7-mq02: 172.18.1.153 CentOS7-mq03: 172.18.1.154 CentOS7-kibana: 172.18.10.106 elasticsearch-6.2.3.tar.gz logstash-6.2.3.tar.gz kibana-6.2.3-linux-x86_64.tar.gz 创建用户 elk12useradd elkpasswd elk 集群搭建Elasticsearch123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[mq01]$ pwd/data/share/download[mq01]$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.3.tar.gz[mq01]$ tar zxvf elasticsearch-6.2.3.tar.gz -C /usr/local/server/[mq01]$ cd /usr/local/server/# 更改所属用户[mq01]$ chown -R elk:elk elasticsearch-6.2.3/[mq01]$ cd elasticsearch-6.2.3/[mq01]$ lldrwxr-xr-x 3 elk elk 4096 4月 14 00:37 bindrwxr-xr-x 3 elk elk 4096 4月 14 00:37 configdrwxr-xr-x 2 elk elk 4096 3月 13 18:08 lib-rw-r--r-- 1 elk elk 11358 3月 13 18:02 LICENSE.txtdrwxr-xr-x 2 elk elk 4096 4月 14 00:43 logsdrwxr-xr-x 16 elk elk 4096 3月 13 18:08 modules-rw-r--r-- 1 elk elk 191887 3月 13 18:07 NOTICE.txtdrwxr-xr-x 3 elk elk 4096 4月 14 00:37 plugins-rw-r--r-- 1 elk elk 9268 3月 13 18:02 README.textile[mq01]$ cd config/[mq01]$ ll-rw-rw---- 1 elk elk 2938 4月 14 00:32 elasticsearch.yml-rw-rw---- 1 elk elk 2767 3月 13 18:02 jvm.options-rw-rw---- 1 elk elk 5091 3月 13 18:02 log4j2.properties[mq01]$vi elasticsearch.ymlcluster.name: dev-elasticsearchnode.name: node-1node.attr.rack: r1path.data: /data/share/server_data/mq_01/elasticsearchpath.logs: /data/share/server_log/mq_01/elasticsearchnetwork.host: 172.18.1.152http.port: 9200discovery.zen.ping.unicast.hosts: ["172.18.1.152", "172.18.1.153", "172.18.1.154"]discovery.zen.minimum_master_nodes: 2gateway.recover_after_nodes: 3# 安装X-Pack for Elasticsearch[mq01]$ cd ../bin[mq01]$ ./elasticsearch-plugin install x-pack# 切换为elk用户，以后台进程启动[mq01]$ chown -R elk:elk /data/share/server_data/mq_01/elasticsearch[mq01]$ chown -R elk:elk /data/share/server_log/mq_01/elasticsearch[mq01]$ su elk[mq01]$ nohup ./elasticsearch &gt; ../logs/run.log 2&gt;&amp;1 &amp;# 设置密码[mq01]$ ./x-pack/setup-passwords interactive mq02、mq03相同的安装步骤，文件路径和日志输出路径做相应修改 Elasticsearch集群搭建 Elasticsearch官方文档 X-pack for Elasticsearch 异常处理 logstash1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[mq01]$ pwd/data/share/download[mq01]$ wget https://artifacts.elastic.co/downloads/logstash/logstash-6.2.3.tar.gz[mq01]$ tar zxvf logstash-6.2.3.tar.gz -C /usr/local/server/[mq01]$ cd /usr/local/server/# 更改所属用户[mq01]$ chown -R elk:elk logstash-6.2.3/[mq01]$ cd logstash-6.2.3/[mq01]$ lldrwxr-xr-x 2 elk elk 4096 4月 13 22:21 bindrwxr-xr-x 2 elk elk 4096 4月 13 23:47 config-rw-r--r-- 1 elk elk 2276 3月 13 19:49 CONTRIBUTORSdrwxr-xr-x 2 elk elk 4096 3月 13 19:49 data-rw-r--r-- 1 elk elk 3869 3月 13 19:53 Gemfile-rw-r--r-- 1 elk elk 21170 3月 13 19:51 Gemfile.lockdrwxr-xr-x 6 elk elk 4096 4月 13 22:21 lib-rw-r--r-- 1 elk elk 589 3月 13 19:49 LICENSEdrwxr-xr-x 4 elk elk 4096 4月 13 22:21 logstash-coredrwxr-xr-x 3 elk elk 4096 4月 13 22:21 logstash-core-plugin-apidrwxr-xr-x 4 elk elk 4096 4月 13 22:21 modules-rw-rw-r-- 1 elk elk 28122 3月 13 19:53 NOTICE.TXTdrwxr-xr-x 3 elk elk 4096 4月 13 22:21 toolsdrwxr-xr-x 4 elk elk 4096 4月 13 22:21 vendor[mq01]$ cd config/[mq01]$ ll-rw-r--r-- 1 elk elk 1894 4月 13 23:27 jvm.options-rw-r--r-- 1 elk elk 4466 3月 13 19:49 log4j2.properties-rw-r--r-- 1 elk elk 6381 4月 13 23:39 logstash.yml-rw-r--r-- 1 elk elk 3244 3月 13 19:49 pipelines.yml-rw-r--r-- 1 elk elk 1802 4月 13 23:47 startup.options# 安装X-Pack for Logstash[mq01]$ cd ../bin[mq01]$ ./logstash-plugin install x-pack[mq01]$ vi logstash.ymlpath.data: /data/share/server_data/mq_01/logstashpath.logs: /data/share/server_log/mq_01/logstash# 数量等于CPU数pipeline.workers: 4# 数据落盘queue.type: persisted[mq01]$ vi startup.optionsJAVACMD=/usr/local/server/jdk1.8.0_151/bin/javaLS_HOME=/usr/local/server/logstash-6.2.3LS_USER=elkLS_GROUP=elkLS_PIDFILE=/data/share/server_log/mq_01/logstash/logstash.pidLS_GC_LOG_FILE=/data/share/server_log/mq_01/logstash/gc.log[mq01]$ chown -R elk:elk /data/share/server_data/mq_01/logstash[mq01]$ chown -R elk:elk /data/share/server_log/mq_01/logstash# 测试logstash[mq01]$ ../bin/logstash -e 'input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;' mq02、mq03相同的安装步骤，文件路径和日志输出路径做相应修改 ELK中文文档logstash官方文档]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>ELK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka 学习笔记（一）]]></title>
    <url>%2F2018%2F04%2F12%2FKafka-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Kafka 简介定位：分布式流处理平台。 3个关键特性 能够允许发布和订阅流数据。 存储流数据时提供相应的容错机制。 当流数据到达时能够被及时处理。 基本概念1、 主题 Topic​​ 一组消息抽象归纳为一个主题 2、消息 Message kafka通信的基本单位，由一个固定长度的消息头和一个可变长度的消息体构成。 3、分区 Partition 每个主题分成一个或多个分区，每个分区由一系列有序、不可变的消息组成，是一个有序队列。 kafka只能保证一个分区之内消息的有序性。 4、副本 Replica 每个分区在物理上对应一个文件夹，每个分区又有一至多个副本，分区的副本分布在集群的不同代理上，以提高可用性。 5、偏移量 任何发布到分区的消息会被直接追加到日志文件的尾部，而每条消息在日志文件中的位置都会对应一个按序递增的偏移量。 偏移量是一个分区下严格有序的逻辑值，不表示消息在磁盘上的物流位置。 6、日志段 LogSegment 一个日志被分为多个日志段。日志段是Kafka日志对象分片的最小单位。 7、代理 Broker kafka实例称为代理 broker。 8、生产者 Producer 生成者负责将消息发送给代理。 9、消费者 Consumer 消费组 ConsumerGroup 消费者以拉取(pull)方式拉取数据。 如果不指定消费组，则该消费者属于默认消费组(test-consumer-group)。 同一个主题的一条消息只能被同一个消费组下某一个消费者消费，但不同消费组的消费者可同时消费该消息。 Kafka特性1、消息持久化 以文件系统来存储数据，磁盘顺序追加，采用时间复杂度O(1)的磁盘结构，提供了常量时间的性能。 2、高吞吐量 磁盘顺序读写，数据写入及数据同步采用了零拷贝技术，采用sendFile()函数调用，完全在内核中操作，支持数据压缩和批量发送。 3、扩展性 依赖Zookeeper来对集群进行协调管理。 4、多客户端支持 5、Kafka Stream 6、安全机制 7、数据备份 8、轻量级 9、消息压缩 支持Gzip、Snappy、LZ4 三种压缩方式，通常把多条消息放在一起组成MessageSet，然后在把MessageSet放到一条消息里面去，从而提高压缩比率进而提高吞吐量。]]></content>
      <categories>
        <category>高并发架构</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F04%2F12%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
